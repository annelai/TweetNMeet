{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TweetNMeet Bot\n",
    "\n",
    "Hackathon 2017\n",
    "Christine Chung & Anne Lai\n",
    "\n",
    "## Set-up\n",
    "\n",
    "- Install nltk, gensim, twython\n",
    "- Get OAuth credentials from Twitter\n",
    "- mkdir ~/twitter-files\n",
    "- Create a credentials.txt file in twitter-files/\n",
    "- Add your oauth info in this format: \n",
    "```\n",
    "app_key=YOUR CONSUMER KEY  \n",
    "app_secret=YOUR CONSUMER SECRET  \n",
    "oauth_token=YOUR ACCESS TOKEN  \n",
    "oauth_token_secret=YOUR ACCESS TOKEN SECRET\n",
    "```\n",
    "- Export to environment: `export TWITTER=\"/path/to/your/twitter-files\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from twython import Twython\n",
    "from nltk.twitter import Twitter, Query, Streamer, credsfromfile\n",
    "import json, re, pickle, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "We primarily used Twython for interacting with the Twitter API, though NLTK has some neat built-in tools as well that we used for initial data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tw = Twitter()\n",
    "# #Writing meetup tweets out\n",
    "# tw.tweets(keywords='enjoy', to_screen=False, stream=False, limit=20000) #sample from the public stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We chose 10 from Meetup's 36 broad categories\n",
    "categories = {\"Technology\": None,\n",
    "              \"Outdoors\": None,\n",
    "              \"Arts\": None,\n",
    "              \"Books\": None,\n",
    "              \"Business\": None,\n",
    "              \"Language\": None,\n",
    "              \"Sports and Fitness\": None,\n",
    "              \"Food\": None,\n",
    "              \"LGBTQ\": None,\n",
    "              \"Fashion\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oauth = credsfromfile()\n",
    "client = Twython(**oauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userIDs = {}\n",
    "for cat in categories:\n",
    "    users = client.search_users(q=cat)\n",
    "    userIDs[cat] = [user['id'] for user in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in categories:\n",
    "#     if categories[category] is not None:\n",
    "#         continue\n",
    "#     print(category)\n",
    "#     IDs = userIDs[category]\n",
    "#     tweets = [client.get_user_timeline(user_id=thisID, count=200) for thisID in IDs]\n",
    "#     categories[category] = [item['text'] for sublist in tweets for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tweets in Technology: 3999\n",
      "# Tweets in Outdoors: 4000\n",
      "# Tweets in Arts: 4000\n",
      "# Tweets in Books: 4000\n",
      "# Tweets in Business: 4000\n",
      "# Tweets in Sports and Fitness: 3985\n",
      "# Tweets in Food: 4000\n",
      "# Tweets in LGBTQ: 3966\n",
      "# Tweets in Fashion: 3998\n",
      "# Tweets in Language: 3995\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    if categories[category] is None:\n",
    "        print(category)\n",
    "    else:\n",
    "        print(\"# Tweets in {}: {}\".format(category, len(categories[category])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english') + ['amp', 'rt', '—'])\n",
    "lemma = WordNetLemmatizer()\n",
    "translator = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "def clean(doc):\n",
    "    punc_free = doc.translate(translator)\n",
    "    stop_free = \" \".join([i for i in punc_free.lower().split() if i not in stop])\n",
    "    url_free = re.sub(r\"http\\S+\", '', stop_free)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in url_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTweets = [tweet for cat in categories for tweet in categories[cat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweetCorpus.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(allTweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean and produce bag of words from a list of tweets\n",
    "def getDocBoW(docs):\n",
    "    doc_clean = [clean(doc).split() for doc in docs]\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    return  [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix = getDocBoW(allTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LDA Model\n",
    "\n",
    "[wiki](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = lda(doc_term_matrix, num_topics=20, id2word = dictionary, passes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ldamodel, open(\"ldamodel.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"rt\" + 0.011*\"recipe\" + 0.010*\"good\" + 0.008*\"latin\" + 0.005*\"full\" + 0.005*\"man\" + 0.005*\"music\" + 0.004*\"yes\" + 0.004*\"health\" + 0.004*\"world\"'),\n",
       " (1,\n",
       "  '0.018*\"rt\" + 0.013*\"please\" + 0.013*\"lgbtq\" + 0.012*\"transgender\" + 0.012*\"spanish\" + 0.011*\"german\" + 0.010*\"irish\" + 0.010*\"italian\" + 0.007*\"trans\" + 0.006*\"—\"'),\n",
       " (2,\n",
       "  '0.056*\"word\" + 0.054*\"day\" + 0.015*\"rt\" + 0.014*\"thank\" + 0.008*\"japanese\" + 0.005*\"danielnewman\" + 0.005*\"walkrstalkrcon\" + 0.005*\"celebrate\" + 0.004*\"sorry\" + 0.004*\"game\"'),\n",
       " (3,\n",
       "  '0.043*\"audio\" + 0.024*\"rt\" + 0.014*\"amp\" + 0.011*\"u\" + 0.008*\"korean\" + 0.008*\"join\" + 0.007*\"learn\" + 0.007*\"tonight\" + 0.007*\"story\" + 0.007*\"place\"'),\n",
       " (4,\n",
       "  '0.014*\"fashion\" + 0.010*\"book\" + 0.010*\"rt\" + 0.008*\"lgbt\" + 0.007*\"community\" + 0.007*\"amp\" + 0.006*\"come\" + 0.006*\"new\" + 0.005*\"perfect\" + 0.004*\"time\"'),\n",
       " (5,\n",
       "  '0.020*\"food\" + 0.019*\"rt\" + 0.011*\"amp\" + 0.009*\"english\" + 0.007*\"eat\" + 0.007*\"art\" + 0.007*\"scoop\" + 0.006*\"4\" + 0.005*\"free\" + 0.005*\"child\"'),\n",
       " (6,\n",
       "  '0.012*\"photo\" + 0.011*\"rt\" + 0.009*\"new\" + 0.009*\"time\" + 0.009*\"see\" + 0.008*\"check\" + 0.008*\"russian\" + 0.007*\"list\" + 0.007*\"tip\" + 0.007*\"every\"'),\n",
       " (7,\n",
       "  '0.028*\"language\" + 0.011*\"summer\" + 0.010*\"rt\" + 0.009*\"learning\" + 0.007*\"next\" + 0.006*\"plan\" + 0.006*\"3\" + 0.006*\"get\" + 0.006*\"say\" + 0.005*\"2016\"'),\n",
       " (8,\n",
       "  '0.042*\"click\" + 0.042*\"rt\" + 0.013*\"thanks\" + 0.012*\"day\" + 0.011*\"great\" + 0.009*\"week\" + 0.008*\"amp\" + 0.008*\"swedish\" + 0.007*\"book\" + 0.007*\"best\"'),\n",
       " (9,\n",
       "  '0.013*\"amp\" + 0.011*\"vegan\" + 0.011*\"sport\" + 0.009*\"weekend\" + 0.008*\"rt\" + 0.008*\"goifex\" + 0.007*\"shop\" + 0.007*\"love\" + 0.007*\"season\" + 0.005*\"gay\"')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial and Error\n",
    "\n",
    "The first time we trained this model, we trained it on two corpora: tweets containing the word \"meetup\", and tweets containing the word \"enjoy\". These, it turns out, were not incredibly helpful. But it did surface some pretty great topics, including a topic dominated by the words \"miserable\", \"island\", \"channel4news\", and \"brexit\". It also surfaced topics related to Darren Criss (performing on the today show?), grilling, and happy women.\n",
    "\n",
    "We later sampled tweets based on 10 meetup categories that we handpicked. However, the original results are here for your enjoyment.\n",
    "\n",
    "```\n",
    "[(0,\n",
    "  '0.248*\"enjoy\" + 0.011*\"yesterday\" + 0.011*\"love\" + 0.011*\"created\" + 0.010*\"michael\" + 0.009*\"summer\" + 0.009*\"u\" + 0.009*\"everyone\" + 0.007*\"5\" + 0.007*\"im\"'),\n",
    " (1,\n",
    "  '0.014*\"miserable\" + 0.013*\"island\" + 0.013*\"moore\" + 0.013*\"channel4news\" + 0.011*\"via\" + 0.011*\"https…\" + 0.010*\"new\" + 0.009*\"here\" + 0.008*\"brexit\" + 0.008*\"job\"'),\n",
    " (2,\n",
    "  '0.133*\"enjoy\" + 0.032*\"happy\" + 0.023*\"amp\" + 0.022*\"it\" + 0.021*\"dont\" + 0.016*\"long\" + 0.016*\"woman\" + 0.015*\"doesnt\" + 0.015*\"together\" + 0.013*\"🤷🏽\\u200d♀️\"'),\n",
    " (3,\n",
    "  '0.029*\"time\" + 0.027*\"hope\" + 0.021*\"i\" + 0.020*\"day\" + 0.018*\"dream\" + 0.016*\"fantastic\" + 0.015*\"performing\" + 0.015*\"todayshow\" + 0.015*\"darrencriss\" + 0.015*\"dreamed\"'),\n",
    " (4,\n",
    "  '0.016*\"always\" + 0.016*\"new\" + 0.014*\"man\" + 0.013*\"watching\" + 0.013*\"show\" + 0.012*\"time\" + 0.009*\"every\" + 0.009*\"start\" + 0.009*\"live\" + 0.008*\"much\"'),\n",
    " (5,\n",
    "  '0.018*\"best\" + 0.016*\"people\" + 0.012*\"meetup\" + 0.012*\"join\" + 0.010*\"movie\" + 0.009*\"know\" + 0.008*\"u\" + 0.007*\"amp\" + 0.006*\"thats\" + 0.006*\"awesome\"'),\n",
    " (6,\n",
    "  '0.059*\"great\" + 0.054*\"weekend\" + 0.053*\"…\" + 0.046*\"light\" + 0.044*\"grill\" + 0.044*\"promotion\" + 0.044*\"perduechicken\" + 0.044*\"perduecrew\" + 0.044*\"kebab\" + 0.008*\"follow\"'),\n",
    " (7,\n",
    "  '0.092*\"enjoy\" + 0.012*\"go\" + 0.011*\"know\" + 0.008*\"get\" + 0.007*\"amp\" + 0.007*\"stop\" + 0.007*\"week\" + 0.007*\"favorite\" + 0.007*\"cant\" + 0.007*\"u\"'),\n",
    " (8,\n",
    "  '0.038*\"drive\" + 0.030*\"black\" + 0.011*\"think\" + 0.010*\"game\" + 0.010*\"want\" + 0.010*\"year\" + 0.009*\"made\" + 0.009*\"much\" + 0.009*\"thanks\" + 0.008*\"amp\"'),\n",
    " (9,\n",
    "  '0.093*\"life\" + 0.078*\"u\" + 0.068*\"gonna\" + 0.043*\"come\" + 0.039*\"like\" + 0.036*\"girl\" + 0.036*\"me\" + 0.033*\"forget\" + 0.032*\"either\" + 0.031*\"gon\"')]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Topic for a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myTweets = [tweet['text'] for tweet in client.get_user_timeline(\n",
    "                                                            screen_name=\"curmudgeon\", \n",
    "                                                            include_rts=True,\n",
    "                                                            count=200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [val for sublist in list_of_lists for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDocTermMatrix = flatten(getDocBoW(myTweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLDA = ldamodel[myDocTermMatrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.1385202554208125),\n",
       " (1, 0.12840139598610675),\n",
       " (7, 0.12785929686654426),\n",
       " (6, 0.099817503944872488),\n",
       " (5, 0.092628977457324416),\n",
       " (3, 0.088213090228229304),\n",
       " (8, 0.083397223169100226),\n",
       " (2, 0.082450493258694371),\n",
       " (9, 0.082357441773621767),\n",
       " (4, 0.076354321894694002)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLDA.sort(key=lambda x: x[1], reverse=True)\n",
    "myLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/christine/twitter-files/tweets.20170803-160530.json\n",
      "No more Tweets available through rest api\n",
      "Written 6432 Tweets\n"
     ]
    }
   ],
   "source": [
    "#Writing meetup tweets out\n",
    "tw.tweets(keywords='meetup', to_screen=False, stream=False, limit=20000) #sample from the public stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Sports', 'size': 31, 'slug': 'sports'},\n",
       " {'name': 'Entertainment', 'size': 14, 'slug': 'entertainment'},\n",
       " {'name': 'Music', 'size': 15, 'slug': 'music'},\n",
       " {'name': 'Digital Creators', 'size': 15, 'slug': 'digital-creators'},\n",
       " {'name': 'News', 'size': 15, 'slug': 'news'},\n",
       " {'name': 'Gaming', 'size': 15, 'slug': 'gaming'},\n",
       " {'name': 'Government', 'size': 15, 'slug': 'government'},\n",
       " {'name': 'Television', 'size': 13, 'slug': 'television'},\n",
       " {'name': 'Funny', 'size': 14, 'slug': 'funny'},\n",
       " {'name': 'Fashion', 'size': 14, 'slug': 'fashion'},\n",
       " {'name': 'Food & Drink', 'size': 15, 'slug': 'food-drink'},\n",
       " {'name': 'Family', 'size': 9, 'slug': 'family'},\n",
       " {'name': 'Business', 'size': 9, 'slug': 'business'},\n",
       " {'name': 'Books', 'size': 9, 'slug': 'books'},\n",
       " {'name': 'Leaders', 'size': 12, 'slug': 'leaders'},\n",
       " {'name': 'Influencers', 'size': 12, 'slug': 'influencers'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_user_suggestions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Twitter with NLTK](http://www.nltk.org/howto/twitter.html)\n",
    "- [LDA with Gensim & Python Tutorial](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)\n",
    "- [Gensim LDA Docs](https://radimrehurek.com/gensim/models/ldamodel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
